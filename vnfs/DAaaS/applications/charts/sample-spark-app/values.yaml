# Default values for sample-spark-app.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.


#===========================KUBERNETES POD RELATED CONFIGs========================
image: spark-tf-keras-horo:latest
imagePullPolicy: Never
restartPolicy: Never
volumesName: test-volume
hostpath: /tmp
hostpathType: Directory



#============================SPARK APP RELATED CONFIGs=============================

nameOfTheSparkApp: spark-apache-logs2 
# Python or Scala supported.
programmingLanguageType: Scala
modeOfSparkApp: cluster
mainClassOfTheSparkApp: ApacheLogAnalysis
# can be http path, s3 path, minio path
mainApplicationFileOfTheSparkApp: https://github.com/mohanraj1311/ApacheLogAnalysisJar/raw/master/analysisofapachelogs_2.11-0.1.jar 
argumentsOfTheSparkProgram:
    - hdfs://hdfs-1-namenode-1.hdfs-1-namenode.hdfs1.svc.cluster.local:8020/data/apache-logs 



#============================SPARK DRIVER RELATED CONFIGs=========================
driverCores: 0.1
driverCoreLimit: 200m
driverMemory: 1024m
driverVolumeMountsName: test-volume
driverVolumeMountPath: /tmp 



#============================SPARK EXECUTOR RELATED CONFIGs=======================
executorCores: 1 
executorInstances: 1 
executorMemory: 512m
executorVolumeMountsName: test-volume
executorVolumeMountPath: /tmp



#===========================HADOOP RELATED CONFIGs===============================
# config map of the hdfs
hadoopConfigMap: hdfs-1-config


###################################################################################




